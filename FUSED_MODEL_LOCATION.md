# Where is the Fused Model?

## ğŸ¯ Quick Answer

The **fused model** is not a single file - it's a **runtime system** that combines:
1. **Base LLM** (from HuggingFace, e.g., GPT-2, Llama)
2. **AdaptiVocab PatchTokenizer** (you create this)
3. **vAttention memory management** (built into Sarathi-Serve)

---

## ğŸ“ Component Locations

### 1. Integration Code (âœ… Already Done)
**Location**: `vattention/sarathi-lean/sarathi/`

**Key Files:**
- `transformers_utils/patch_tokenizer_wrapper.py` - AdaptiVocab integration
- `transformers_utils/tokenizer.py` - Modified to load PatchTokenizer
- `engine/base_llm_engine.py` - Engine that uses fused system
- `config.py` - Configuration with `patch_tokenizer_path` support
- `entrypoints/openai_server/api_server.py` - **Start server here**

### 2. PatchTokenizer (âš ï¸ You Need to Create)
**Location**: `AdaptiVocab/src/saved_patch_tokenizers_no_ngrams_new_logs/[config_name]/`

**Files created:**
- `patch_tokenizer.pkl` - **This is what you need**
- `config.pkl`
- `removed_tokens.pkl`
- `added_ngrams.pkl`

**How to create:**
```bash
cd AdaptiVocab/src/build_vocab
python3 create_patch_tokenizer.py
```

### 3. Base Model (âœ… Auto-downloaded)
**Location**: HuggingFace cache (auto-downloaded when you specify `--model_name`)

---

## ğŸš€ How to Run the Fused Model

### Step 1: Create PatchTokenizer

```bash
cd AdaptiVocab/src/build_vocab

# Edit create_patch_tokenizer.py to configure:
# - original_tokenizer (e.g., 'gpt2')
# - target_corpus_name (your domain dataset)
# - num_to_add, num_to_remove

python3 create_patch_tokenizer.py
```

**Output**: `AdaptiVocab/src/saved_patch_tokenizers_no_ngrams_new_logs/[config_name]/patch_tokenizer.pkl`

### Step 2: Start Fused Model Server

```bash
cd vattention/sarathi-lean

python -m sarathi.entrypoints.openai_server.api_server \
    --model_name gpt2 \
    --patch_tokenizer_path ../../AdaptiVocab/src/saved_patch_tokenizers_no_ngrams_new_logs/[your_config]/patch_tokenizer.pkl \
    --model_attention_backend fa_vattn \
    --model_block_size 2097152
```

**This starts the fused model!**
- âœ… AdaptiVocab active (via PatchTokenizer)
- âœ… vAttention active (via `fa_vattn` backend)
- âœ… Server running on `http://localhost:8000`

---

## ğŸ“‚ Complete File Structure

```
OS/
â”œâ”€â”€ AdaptiVocab/
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ build_vocab/
â”‚       â”‚   â””â”€â”€ create_patch_tokenizer.py  # â† Create PatchTokenizer here
â”‚       â””â”€â”€ saved_patch_tokenizers_no_ngrams_new_logs/  # â† PatchTokenizer saved here
â”‚           â””â”€â”€ [config_name]/
â”‚               â””â”€â”€ patch_tokenizer.pkl    # â† Your PatchTokenizer file
â”‚
â”œâ”€â”€ vattention/
â”‚   â””â”€â”€ sarathi-lean/
â”‚       â””â”€â”€ sarathi/
â”‚           â”œâ”€â”€ transformers_utils/
â”‚           â”‚   â”œâ”€â”€ patch_tokenizer_wrapper.py  # â† Integration code
â”‚           â”‚   â””â”€â”€ tokenizer.py                # â† Integration code
â”‚           â”œâ”€â”€ engine/
â”‚           â”‚   â””â”€â”€ base_llm_engine.py          # â† Integration code
â”‚           â””â”€â”€ entrypoints/
â”‚               â””â”€â”€ openai_server/
â”‚                   â””â”€â”€ api_server.py           # â† START FUSED MODEL HERE
â”‚
â””â”€â”€ benchmark_comprehensive.py  # Test fused vs normal
```

---

## ğŸ” Finding Your PatchTokenizer

If you've already created one:

```bash
# Search for PatchTokenizer files
find AdaptiVocab -name "patch_tokenizer.pkl" -type f

# Check the default save location
ls -R AdaptiVocab/src/saved_patch_tokenizers_no_ngrams_new_logs/
```

**Default save path** (from `create_patch_tokenizer.py`):
```
AdaptiVocab/src/saved_patch_tokenizers_no_ngrams_new_logs/[config_name]/patch_tokenizer.pkl
```

Where `[config_name]` is generated from your configuration (model name, corpus, etc.)

---

## âš ï¸ Important Notes

### The Fused Model is:
- âœ… **Integration code** - Already in `vattention/sarathi-lean/`
- âœ… **Runtime system** - Starts when you run the server
- âš ï¸ **Requires PatchTokenizer** - You need to create this first
- âš ï¸ **Requires GPU** - For full vAttention benefits (Mac can't run vAttention)

### On Mac (Your System):
- âœ… Can create PatchTokenizer
- âœ… Can test tokenization
- âš ï¸ Cannot run vAttention (needs CUDA/NVIDIA GPU)
- âš ï¸ Can test AdaptiVocab tokenization only

### For Full Fused Model:
- Need GPU (AWS, GCP, or local NVIDIA GPU)
- Then both AdaptiVocab + vAttention work together

---

## ğŸ“ Summary

**The fused model location:**

1. **Integration Code**: `vattention/sarathi-lean/sarathi/` âœ… (already done)
2. **PatchTokenizer**: `AdaptiVocab/src/saved_patch_tokenizers_no_ngrams_new_logs/[config]/patch_tokenizer.pkl` (create it)
3. **Server Entry**: `vattention/sarathi-lean/sarathi/entrypoints/openai_server/api_server.py` âœ… (start here)

**To use it:**
1. Create PatchTokenizer â†’ `AdaptiVocab/src/build_vocab/create_patch_tokenizer.py`
2. Start server â†’ `vattention/sarathi-lean/sarathi/entrypoints/openai_server/api_server.py`
3. Use with `--patch_tokenizer_path` + `--model_attention_backend fa_vattn`

---

**The fused model = Integration code + PatchTokenizer + Running server**




