# How to Use the Fused Model

## üéØ Quick Answer

The **fused model** is not a single file - it's a **running system** that combines:
- **AdaptiVocab** (via PatchTokenizer)
- **vAttention** (via Sarathi-Serve)
- **Base LLM** (e.g., GPT-2, Llama, etc.)

---

## üìç Where Everything Is

### 1. Integration Code (Already Done ‚úÖ)
**Location**: `vattention/sarathi-lean/sarathi/`
- `transformers_utils/patch_tokenizer_wrapper.py` - AdaptiVocab integration
- `engine/base_llm_engine.py` - Engine that uses fused system
- `config.py` - Configuration with `patch_tokenizer_path` support

### 2. PatchTokenizer (You Need to Create)
**Location**: `AdaptiVocab/src/saved_patch_tokenizers_*/[config_name]/`
- Created by: `AdaptiVocab/src/build_vocab/create_patch_tokenizer.py`
- File: `patch_tokenizer.pkl`

### 3. Server Entry Point
**Location**: `vattention/sarathi-lean/sarathi/entrypoints/openai_server/api_server.py`
- This is where you start the fused model

---

## üöÄ Step-by-Step: Using the Fused Model

### Step 1: Create a PatchTokenizer

```bash
cd AdaptiVocab/src/build_vocab

# Edit create_patch_tokenizer.py to set your config, then:
python3 create_patch_tokenizer.py
```

**Output location**: 
```
AdaptiVocab/src/saved_patch_tokenizers_no_ngrams_new_logs/[config_name]/
‚îú‚îÄ‚îÄ patch_tokenizer.pkl      # ‚Üê This is what you need
‚îú‚îÄ‚îÄ config.pkl
‚îú‚îÄ‚îÄ removed_tokens.pkl
‚îî‚îÄ‚îÄ added_ngrams.pkl
```

### Step 2: Start the Fused Model Server

```bash
cd vattention/sarathi-lean

python -m sarathi.entrypoints.openai_server.api_server \
    --model_name gpt2 \
    --patch_tokenizer_path ../../AdaptiVocab/src/saved_patch_tokenizers_no_ngrams_new_logs/[your_config]/patch_tokenizer.pkl \
    --model_attention_backend fa_vattn \
    --model_block_size 2097152
```

**What this does:**
- Loads base model (gpt2)
- Loads PatchTokenizer (AdaptiVocab) ‚úÖ
- Uses vAttention backend ‚úÖ
- **Fused model is now running!**

### Step 3: Use the Fused Model

The server runs on `http://localhost:8000` with OpenAI-compatible API.

**Test it:**
```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "gpt2",
    "prompt": "Hello, world!",
    "max_tokens": 50
  }'
```

---

## üîç Finding Your PatchTokenizer

If you've already created one:

```bash
# Search for it
find AdaptiVocab -name "patch_tokenizer.pkl" -type f

# Check the saved directory
ls -R AdaptiVocab/src/saved_patch_tokenizers*/
```

**Default save location** (from constants.py):
```
AdaptiVocab/src/saved_patch_tokenizers_ngram_k_analysis/
```

Or:
```
AdaptiVocab/src/saved_patch_tokenizers_no_ngrams_new_logs/
```

---

## üìù Example: Complete Workflow

```bash
# 1. Create PatchTokenizer
cd AdaptiVocab/src/build_vocab
python3 create_patch_tokenizer.py
# ‚Üí Creates: saved_patch_tokenizers_no_ngrams_new_logs/[config]/patch_tokenizer.pkl

# 2. Note the path
PATCH_PATH="$(pwd)/../saved_patch_tokenizers_no_ngrams_new_logs/[config]/patch_tokenizer.pkl"

# 3. Start fused model
cd ../../../vattention/sarathi-lean
python -m sarathi.entrypoints.openai_server.api_server \
    --model_name gpt2 \
    --patch_tokenizer_path "$PATCH_PATH" \
    --model_attention_backend fa_vattn \
    --model_block_size 2097152

# 4. Fused model is now running!
# - AdaptiVocab: ‚úÖ Active (via PatchTokenizer)
# - vAttention: ‚úÖ Active (via fa_vattn backend)
```

---

## ‚ö†Ô∏è Important Notes

### The Fused Model Requires:

1. **PatchTokenizer** ‚úÖ (Create using AdaptiVocab)
2. **GPU** ‚ö†Ô∏è (For vAttention - Mac can't run this)
3. **Base Model** ‚úÖ (Downloaded automatically from HuggingFace)

### On Mac (Your System):

- ‚úÖ Can create PatchTokenizer
- ‚úÖ Can test tokenization
- ‚ö†Ô∏è Cannot run vAttention (needs CUDA/NVIDIA GPU)
- ‚ö†Ô∏è Can test AdaptiVocab tokenization only

### For Full Fused Model:

- Need GPU (AWS, GCP, or local NVIDIA GPU)
- Then both AdaptiVocab + vAttention work together

---

## üéØ Summary

**The fused model is:**
- **Code**: Already integrated in `vattention/sarathi-lean/`
- **Tokenizer**: Create with AdaptiVocab ‚Üí `patch_tokenizer.pkl`
- **Server**: Start via `api_server.py` with both enabled
- **Running**: When server starts with `--patch_tokenizer_path` + `--model_attention_backend fa_vattn`

**To use it:**
1. Create PatchTokenizer
2. Start server with PatchTokenizer path
3. Use OpenAI-compatible API

---

**Location Summary:**
- Integration code: `vattention/sarathi-lean/sarathi/` ‚úÖ
- PatchTokenizer: `AdaptiVocab/src/saved_patch_tokenizers*/[config]/patch_tokenizer.pkl` (create it)
- Server: `vattention/sarathi-lean/sarathi/entrypoints/openai_server/api_server.py` ‚úÖ




